Распределенная аналитическая СУБД, основанная на PSQL, но созданная для работы с большими объемами данных и выполнения сложных аналитических запросов использованием массивно-параллельной обработки (MPP). Объединяет кластер в несколько серверов, обеспечивая высокую производительность при обработки данных, что делает ее идеальной для построения корпоративных хранилищ.

## Архитектура
### Основные компоненты архитектуры
#### Мастер (Master Host)
Мастер-узел является центральным координатором и единственной точкой входа в систему Greenplum. Его функции:
- Принимает клиентские подключения и SQL-запросы через стандартные интерфейсы (psql, JDBC, ODBC)
- Выполняет аутентификацию пользователей
- Компилирует, оптимизирует и создает план параллельного выполнения запросов
- Координирует работу всех сегментов кластера
- Агрегирует результаты от сегментов и возвращает их клиенту
- Хранит только метаданные (системные каталоги), не содержит пользовательских данных
#### Резервный мастер (Standby Master)
Обеспечивает отказоустойчивость системы:
- Является "теплым" резервом основного мастера. Он поддерживается в актуальном состоянии и готов к включению, но не обрабатывает запросы, пока не будет активирован.
- Синхронизируется с основным мастером через репликацию транзакционных логов
- Активируется вручную при отказе основного мастера
- В некоторых конфигурациях используется как ETL-сервер
#### Сегменты (Segments)
Независимые экземпляры PSQL, которые выполняют фактическое хранение и обработку данных:
##### Первичные сегменты (Primary Segments)
Это активные сегменты, которые хранят пользовательские данные, получают планы запросов и реализуют их. Они выполняют локальные SQL-операции только со своей частью данных и отправляют результаты мастеру.
- Хранят пользовательские данные и реплику каталога БД
- Обрабатывают DML-операции и несут основную пользовательскую нагрузку
- Каждый хост-сегмент содержит от 1 до 8 сегментов в зависимости от характеристик оборудования
##### Зеркальные сегменты (Mirror Segments)
- Хранят реплики данных с соответствующих первичных сегментов
- Используют синхронную репликацию для обеспечения консистентности
- Автоматически активируются при отказе первичных сегментов
- В нормальном режиме не обрабатывают запросы, создавая минимальную нагрузку

Существует две конфигурации зеркалирования:

**Групповое зеркалирование**
Все зеркальные сегменты основных сегментов одного хоста размещаются вместе на другом хосте.
В случае отказа такая конфигурация приводит к неравномерной нагрузке на хосты. Если один хост становится недоступным, его резервный хост получает двойную нагрузку, так как помимо собственных основных сегментов тоже начинают работать в режиме основных сегментов.

**Распределенное зеркалирование**
Зеркальные сегменты для основных сегментов каждого хоста размещаются на разных резервных хостах — по количеству сегментов на хост.
#### Интерконнект (Interconnect)
Высокоскоростная сетевая инфраструктура, обеспечивающая связь между компонентами кластера(Большинство запросов в базе данных выполняются параллельно на всех сегментах и требуют перемещения данных (data motion) между сегментами):
- Использует стандартную коммутационную структуру Ethernet
- Рекомендуется использование 10-гигабитных или более быстрых соединений
- Несколько независимых interconnect-сетей повышают пропускную способность и отказоустойчивость
#### Базы данных
**База данных (БД)** — это набор данных, физически хранимых вместе. Каждый ADB-кластер может содержать несколько баз данных. Однако клиентские приложения могут обращаться не более чем к одной базе данных ADB в рамках одного соединения — кросс-запросы между различными базами данных невозможны по причине жесткого разделения БД.
#### Схемы
**Схема** — это логическое объединение сущностей внутри одной базы данных. В отличие от БД, схемы разделены не так жестко: пользователи могут обращаться к нескольким схемам одновременно (при наличии соответствующих прав). Схемы могут быть полезны для решения следующих задач:
- Организация одновременного доступа нескольких пользователей к одной базе данных.
- Разбиение базы данных на несколько логических групп. Это может быть особенно полезно при настройке прав доступа к различным объектам БД.
- Выделение различных схем под сторонние приложения — так, чтобы не возникало конфликтов с именами объектов в различных схемах.
#### Таблицы
**Таблица** — это основной объект данных в ADB. В целом, таблицы в ADB идентичны таблицам в любой реляционной СУБД, за исключением [распределения](https://docs.arenadata.io/ru/ADB/current/concept/data-model/tables/distribution.html) табличных строк по различным сегментам кластера. При создании таблицы необходимо выбрать политику распределения данных.
#### Типы таблиц
- **Heap**. Этот тип таблиц используется по умолчанию и рекомендуется для OLTP-нагрузок. Heap-таблицы являются оптимальным выбором в случае частого обновления данных после первоначальной загрузки, а также подходят для однострочных операций `INSERT`, `UPDATE` и `DELETE`.
    
- **Append-optimized**. Эти таблицы предпочтительны для OLAP-нагрузок. Они также отлично подходят для пакетной загрузки данных (bulk data loading). Их рекомендуется использовать при редких обновлениях данных, то есть когда в системе преобладают read-only запросы. В отличие от Heap-таблиц (в которых возможна только строковая ориентация данных — row-oriented), таблицы append-optimized поддерживают две формы ориентации данных:
    - **Строковая (row-oriented)**. Эта модель хранения данных рекомендуется для запросов, в которых одновременно извлекаются все либо большая часть столбцов таблицы.
    
    - **Колоночная (column-oriented)**. Эта модель подходит для вычислений на базе небольшого набора столбцов таблицы. Ее также рекомендуется использовать при регулярных обновлениях незначительной части столбцов.
#### Сжатие данных
Для оптимизации I/O и уменьшения размера данных в кластерах ADB можно использовать сжатие — или компрессию — данных (data compression). Сжатие доступно только для append-optimized таблиц. В ADB доступны два типа сжатия данных:
- **Сжатие данных на уровне таблицы (table-level)** — применяется ко всей таблице. Доступно для append-optimized таблиц как со строковой (row-oriented), так и с колоночной (column-oriented) ориентацией данных.
- **Сжатие данных на уровне столбца (column-level)** — применяется к отдельному столбцу. Позволяет использовать различные алгоритмы сжатия для разных столбцов одной таблицы. Этот тип сжатия доступен только для append-optimized таблиц с колоночной ориентацией данных (column-oriented).
Независимо от уровня, на котором применяется сжатие данных, для его настройки можно использовать следующие параметры (storage directives). Они заполняются в выражении `WITH` или `ENCODING`, соответственно:
- `compresstype` — тип сжатия данных. Возможные значения: `ZLIB`, `ZSTD` и `RLE_TYPE`. Значения не чувствительны к регистру. По умолчанию используется значение `none`, при котором сжатие не применяется.
    
- `compresslevel` — уровень сжатия данных. Возможные значения для различных алгоритмов приведены в таблице ниже. Уровни с наименьшими номерами соответствуют самой быстрой, но при этом наименьшей компрессии данных.
### Обзор архитектуры:SMP и MPP
**SMP** - симметричная многороцессорная архитектура. Особенностью является наличие общей физической памяти, разделяемой между всеми процессорами
### Принципы работы MPP-архитектуры
**MPP** - массивная параллельная архитектура. Физическая память распределена. Система состоит из отдельных модулей. 
#### Концепция Shared Nothing
Greenplum реализует архитектуру **"Shared Nothing"**, где каждый узел кластера имеет:

- Собственную память
    
- Собственную операционную систему
    
- Собственные жесткие диски
    
- Независимые процессы обработки
#### Процессы обработки запросов
При выполнении запроса создаются специализированные процессы:

- **QD (Query Dispatcher)** - диспетчер запросов на мастере, координирующий выполнение
    
- **QE (Query Executor)** - исполнители на сегментах, обрабатывающие подзадачи запроса
    
- Взаимодействие между QD и QE происходит через протокол libpq и interconnect
Кластер GP разворачивается на наборе машин состоящим из: 
- Одного мастер сервера и (опционально) одного резервного мастер сервиса.
- Не менее 2х одинаковых сегментных серверов.
Имеют сеть интерконнект.
Мастер-сервер является единой точкой входа для работы с СУБД. На мастер-сервера размещается одна копия СУБД Postgres, именуемая мастер сегментом. Слушает Порт
### Распределение и партиционирование данных
**Распределение данных (distribution)** — одна из самых важных концепций, лежащих в основе ADB. Она означает хранение данных каждой таблицы на различных сегментах кластера. При этом чем более равномерно данные распределяются между сегментами, тем выше производительность всего кластера. Для этого сегменты должны содержать примерно одинаковые порции данных.
#### Распределение данных (Distribution)
Данные равномерно распределяются по сегментам с использованием трех стратегий:
- **Hash Distribution**  - распределение по хэшу указанного столбца (по умолчанию)
    
- **Random Distribution** -  случайное распределение методом round-robin
    
- **Replicated Distribution** - полное дублирование таблицы на всех сегментах
#### Партицирование
**Партиционирование (partitioning)** — это способ повышения производительности запросов за счет логического разбиения больших таблиц (например, таблиц фактов) на небольшие части, называемые **партициями (partitions)**. Партиционирование позволяет оптимизаторам запросов сканировать ограниченное число строк в таблице (на основе условий предикатов) вместо чтения всего содержимого таблицы.

Принадлежность каждой новой записи таблицы к той или иной партиции определяется на основе значения **ключа партиционирования (partition key)** — столбца (или набора столбцов в случае многоуровневого партиционирования), который выбирается при создании партиционированной таблицы.

Партиционирование выполняется на логическом уровне. В отличие от [распределения данных](https://docs.arenadata.io/ru/ADB/current/concept/data-model/tables/distribution.html), партиционирование не делит таблицу физически. Как партиционированные, так и непартиционированные таблицы распределяются между сегментами кластера.
#### Tablespace
**Табличное пространство (tablespace)** — это системный объект, позволяющий администраторам размещать объекты баз данных (таблицы, индексы, spill-файлы и другое) в различных директориях и даже на различных дисках. Другими словами, tablespace определяет, где хранить физические файлы для логических объектов БД. Благодаря наличию tablespace возможно иметь несколько файловых систем на одной машине и выбирать наиболее оптимальный способ хранения для различных данных. Например, часто используемые таблицы можно хранить на высокопроизводительных SSD-дисках, а прочие таблицы (в первую очередь, с архивными данными) — на стандартных HDD-дисках.
#### Spill-файлы
**Spill-файлы** (также известные как **work-файлы**) создаются на диске, если оперативной памяти становится недостаточно для хранения временных данных, формируемых при выполнении запросов к БД. Временные данные могут включать в себя хеши `JOIN`, результаты сортировки, перераспределенные таблицы и так далее.
### Оптимизация запросов
#### GPORCA (Greenplum Optimizer)
Современный оптимизатор запросов, специально разработанный для MPP-архитектуры:
- Является cost-based оптимизатором для больших данных
    
- Генерирует оптимальные планы выполнения с учетом стоимости перемещения данных
    
- Особенно эффективен для партиционированных таблиц, CTE и подзапросов

#### Особенности выполнения запросов
**Motion Operations** - операции перемещения данных между сегментами:
- **Broadcast Motion** - отправка данных одной таблицы на все сегменты (дорогая операция)
    
- **Redistribute Motion** - перераспределение данных по ключу (также дорогая)
    
- **Gather Motion**
- Наличие Motion в плане запроса - сигнал для оптимизации схемы или запроса

### Отказоустойчивость
#### Автоматическое переключение сегментов
При отказе первичного сегмента система автоматически:
- Переводит соответствующий зеркальный сегмент в роль первичного
    
- Продолжает обработку пользовательских запросов
    
- Откатывает транзакции, которые выполнялись на момент отказа
    
- Поддерживает работоспособность до восстановления отказавшего сегмента
#### Восстановление сегментов
Администратор может восстанавливать отказавшие сегменты без остановки системы:
- Процесс восстановления копирует только изменения, пропущенные во время простоя
    
- После синхронизации сегменты возвращаются в предпочтительные роли
#### Особенности эффективности
MPP-архитектура Greenplum обусловливает следующие характеристики работы:
- **Скорость работы ограничивается самым медленным сегментом** - несбалансированное распределение данных снижает производительность всего кластера
    
- **Один запрос на сегменте не может использовать более одного процессорного ядра**
    
- **Идентичная настройка хост-сегментов** критически важна для равномерного распределения нагрузки
    
- **Использование индексов должно быть ограниченным** - из-за распределенной природы данных полное сканирование таблиц часто более эффективно для аналитических запросов